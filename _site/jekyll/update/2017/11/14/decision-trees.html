<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Decision Trees</title>
  <meta name="description" content="One of the most basic algorithm which is easier to understand yet very powerful is Decision Trees. There are two most popular algorithm associated with train...">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/jekyll/update/2017/11/14/decision-trees.html">
  <!-- <link rel="alternate" type="application/rss+xml" title="My Experiments with Machine Learning" href="/feed.xml"> -->
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script>
          MathJax.Hub.Config({
              config: ["MMLorHTML.js"],
              extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js"],
              jax: ["input/TeX"],
              tex2jax: {
                  inlineMath: [ ['$','$'], ["\\(","\\)"] ],
                  displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
                  processEscapes: false
              },
              TeX: {
                  TagSide: "right",
                  TagIndent: ".8em",
                  MultLineWidth: "85%",
                  equationNumbers: {
                     autoNumber: "AMS",
                  },
                  unicode: {
                     fonts: "STIXGeneral,'Arial Unicode MS'"
                  }
              },
              showProcessingMessages: false
          });
  </script>
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">My Experiments with Machine Learning</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Decision Trees</h1>
    <p class="post-meta">
      <time datetime="2017-11-14T00:15:12-05:00" itemprop="datePublished">
        
        Nov 14, 2017
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>One of the most basic algorithm which is easier to understand yet very powerful is Decision Trees. There are two most popular algorithm associated with training the Decision Trees are :</p>
<ol>
  <li>C4.5, ID3</li>
  <li>CART</li>
</ol>

<h3 id="c45">C4.5</h3>
<p>This was developed by Quinlan and is extension of its earlier algorithm ID3. Consider Samples of data s1,s2, ….., sn. Each sample is k-length vector - which means that each sample is represted by k -columns or k variables.</p>

<p>At each node C4.5 chooses the best attribute on which it can be split. The splitting criterion is information gain, which means that whichever attribute gives the largest information gain after splitting, that would be chosen. First, we should what is gain.</p>

<script type="math/tex; mode=display">Gain(S,A) = H(S) - \sum_{a\subset A} \frac{|a|}{|S|} * H(a)</script>

<p>where H is entropy, S is sample and A is the attribute chosen.
Entropy is given by :</p>

<script type="math/tex; mode=display">H(P) = \sum_i-p_i*log_{2}(p_i)</script>

<p>Let’s take an example to make it more clear. The table below shows, age, income and whether that person buys a certain thing or not.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{array}{|c|c|c|}
\hline
Age & Income & Buys \\ \hline
<30& high & yes \\ \hline
>30& medium & yes \\ \hline
>30& low & no \\ \hline
>30& high & no \\ \hline
<30& high & yes \\ \hline
<30& medium & yes \\ \hline
>30& high & no \\ \hline
<30& low & yes \\ \hline
<30& low & yes \\ \hline
 \hline
\end{array} %]]></script>

<p>1st we find the information stored in this table:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
 H[buys]& = -(probability of yes)* log_2(probability of yes) + -(probability of no)* log_2(probability of no) \\
& = -6/9 * log_2(6/9) - 3/9*log_2(3/9) \\
& = 0.91829
  \end {align} %]]></script>

<p>Lets consider now that we split based on income. So we have three categories - low(3), medium(2) and high(4).</p>

<p>So Entropy associated with this attribute is as follows:</p>
<ol>
  <li>For high (2 yes and 2 no) : -2/4 log 2/4 -2/4 log 2/4 = 1</li>
  <li>For medium (two yes) : -2/2 * log 2/2 = 0</li>
  <li>For low (2 yes and 1 no) : -1/3 log (1/3) - 2/3 log(2/3) = 0.9182</li>
</ol>

<p>Therefore, we can now find the gain as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
Gain & = 0.91829 - (4/9 * (1) + 2/9 * 0 + 3/9 * 0.9182)\\
  & = 0.167
  \end {align} %]]></script>

<p>Similarly, the information gain for splitting on age attribute is = 0.5577. As we can see that the information gain is more in case of age, thus it will first choose to split via age attribute. Intuitively, this makes sense, if you see the table again, for age &lt; 30 the person is buying the product.</p>

<h3 id="cart">CART</h3>

<p>This is similar to C4.5 but uses Gini index instead of entropy for gain. Gini index is defined as :</p>

<script type="math/tex; mode=display">Gini(X) = 1 - \sum_x p(x)^2</script>

<p>And there Gain is defined in this case as :</p>

<script type="math/tex; mode=display">Gain(S,A) = Gini(S) - \sum_{a\subset A} \frac{|a|}{|S|} * Gini(a)</script>

<h3 id="when-should-we-stop-growing-the-tree">When Should we stop growing the Tree?</h3>
<p>We should be vary of the depth of the decision tree that we are making. More depth generally means that we are overfitting the data. There are two basic ways to avoid overfitting in Decision Trees:</p>
<ol>
  <li>Pre - Pruning - as the name suggests, we start making decision trees only after making some initial decisions. It can have two major criteria :
    <ul>
      <li>Depth -  which means that we define the maximum depth till which we will allow tree to grow</li>
    </ul>
    <ul>
      <li>Pre defined Conditions - which means, we will make certain conditions and can make our decision trees grow to certain depth till that condition is reached on the leaf node - Following are couple of examples of such pruning conditions:
        <ul>
          <li>if all attribute value on the leaf node are same</li>
          <li>number of attributes in the leaf node are x (where x &lt; total attributes provided initially)</li>
          <li>attributes reaches certain threshold values</li>
          <li>Chi-square feature is not statistically significant</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Post - Pruning - This is pruning mechanism in a bottoms-up approach.</li>
</ol>

<p>C4.5 and CART both uses different pruning mechanism to remove overfitting. C4.5 uses “Reduce Error Pruning” whereas CART uses “Cross validation for selecting Gini Threshold”</p>

<h3 id="reduce-error-pruning">Reduce Error Pruning</h3>
<p>In this mechanism, We start pruning in an bottom’s up approach in the following fashion:
Let T be subtree whose root node is v. We define gain from pruning as :</p>

<script type="math/tex; mode=display">Gain(pruning) = \#misclassification_T - misclassification_v</script>

<p>If this gain is +ve, it means that there were fewer misclassification when we started with and have more misclassification in the subtree T. Thus we will start removing those nodes which have Gain +ve and thereby keeping only -ve pruning gain nodes.</p>

<h3 id="cross-validation-for-selecting-gini-threshold">Cross validation for selecting Gini Threshold</h3>

<p>For cross validation technique you can look at <a href="http://localhost:4000//jekyll/update/2017/11/11/important-terms.html">my previous post</a>. This is pre-pruning mechanism with condition testing. In this method, for each validation test set, define a Gini threshold <script type="math/tex">G_t</script> for the decision tree. This means that whenever the leaf node reaches Gini index at <script type="math/tex">G_t</script>, you should stop. The best <script type="math/tex">G_t</script> is chosen from these validation sets and is finally used during testing.</p>

  </div>

  
    


  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">My Experiments with Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              My Experiments with Machine Learning
            
            </li>
            
            <li><a href="mailto:chawla9@purdue.edu">chawla9@purdue.edu</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/anieshchawla"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">anieshchawla</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/aniesh_chawla"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">aniesh_chawla</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>I started ML this year because of my interest and I wanted to share my experience with people who have been trying to work in this field. This is also a repository for me to revise concepts which I might forget.</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
