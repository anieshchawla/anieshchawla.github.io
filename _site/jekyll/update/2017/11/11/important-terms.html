<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Starting Terms ML</title>
  <meta name="description" content="We will start with some very common terms that have find it use in ML">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://localhost:4000/jekyll/update/2017/11/11/important-terms.html">
  <!-- <link rel="alternate" type="application/rss+xml" title="My Experiments with Machine Learning" href="/feed.xml"> -->
  <script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
       <script type="text/x-mathjax-config">
         MathJax.Hub.Config({
           tex2jax: {
             inlineMath: [ ['$','$'], ["\\(","\\)"] ],
             processEscapes: true
           }
         });
       </script>
  <script type="text/javascript" async
       src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">My Experiments with Machine Learning</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Starting Terms ML</h1>
    <p class="post-meta">
      <time datetime="2017-11-11T00:08:12-05:00" itemprop="datePublished">
        
        Nov 11, 2017
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <p>We will start with some very common terms that have find it use in ML</p>

<h3 id="loss-functions-">Loss functions :</h3>
<p>This is finally what you would expect to be low. There are many loss functions that are used:</p>

<ul>
  <li>L1 Loss - This is basically absolute value of difference of input and output (I have never used) :</li>
</ul>

<script type="math/tex; mode=display">Loss = \sum_{i=0}^n |y_i - f(x_i)|</script>

<ul>
  <li>L2 Loss - This is square of difference of input and output (used extensively) :</li>
</ul>

<script type="math/tex; mode=display">Loss = \sum_{i=0}^n (y_i - f(x_i))^2</script>

<ul>
  <li>Softmax loss - This is loss that comes from softmax classifiers (will discuss it in next post)
  basic equation for this is given by following equation. If <script type="math/tex">y_i</script> is the expected output for the given input.</li>
</ul>

<script type="math/tex; mode=display">Loss = -\log(\frac{\exp(y_i)}{\sum_{k=0}^n \exp(y_k)})</script>

<figure class="highlight"><pre><code class="language-python" data-lang="python">  <span class="c"># Code For Softmax Loss function calculation - used very frequently in Deep Neural Networks</span>
  <span class="c"># Training Inputs:</span>
   <span class="c"># W: A numpy array of shape (D, C) containing weights.</span>
   <span class="c"># X: A numpy array of shape (N, D) containing a minibatch of data.</span>
   <span class="c"># y: A numpy array of shape (N,) containing training labels; y[i] = c means</span>
   <span class="c"># that X[i] has label c, where 0 &lt;= c &lt; C.</span>
   <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_train</span><span class="p">):</span>
      <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
      <span class="c"># doing the following to maintain numerical stability</span>
      <span class="n">scores</span><span class="o">-=</span><span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

      <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
      <span class="n">expscore</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
      <span class="n">scores</span><span class="o">/=</span><span class="n">expscore</span>
      <span class="n">loss</span><span class="o">-=</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]])</span>
   <span class="c">#=&gt; Please note this is not the final loss output - have to take mean and take into account regularisation</span>
   <span class="c">#=&gt; Gives non-vectorized loss for softmax classifier</span>
   </code></pre></figure>

<p>Since we are using Numpy, we should do this in much smarter way i.e. vectorized solution</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python">      <span class="c">#Inputs are same as above</span>
      <span class="n">scores</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
      <span class="c"># following is done to maintain the numerical stability by subtracting the max values</span>
      <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">T</span> <span class="o">-</span> <span class="n">scores</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)),</span><span class="n">scores</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)])</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
      <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
      <span class="n">expval</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
      <span class="n">scores</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">T</span><span class="o">/</span><span class="n">expval</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">()</span>
      <span class="c">#scores[np.arange(len(scores)),y] - this is basically taking the values only belonging to classifier</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scores</span><span class="p">)),</span><span class="n">y</span><span class="p">])</span>
      <span class="c">#=&gt; Gives vectorized loss for softmax</span>
      <span class="c">#=&gt; Please note this is not the final loss output - have to take into account regularisation</span>
      </code></pre></figure>

<h3 id="hyper-parameters">Hyper parameters:</h3>

<p>The parameters that are not learned by during the training process but are required to make your model
learn and converge. Some of the common hyper parameters are :</p>
<ol>
  <li>Learning rate - This tell how fast do you want your model to train. A high value can explode your data
very quickly and a low value will lead to very slow convergence. To select the right value, a number of
iterations are required. A good value to start with is 1e-3. A snippet of how to use learning rate:
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c">#self.W is weight matrix</span>
 <span class="c">#grad is gradient for iteration - will talk more about it on later posts</span>
 <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span>  <span class="p">(</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>Regularisation Strength - Regularisation is responsible to keep your model simple and not to become complex. Since we are dealing with non-linear functions, we need to keep the keep the model in check to not to become super
complex. Generally we use L2 regularization
<script type="math/tex">\sum_i\sum_k W_iW_k</script>. A good value to start with for regularisation strength is 1e-5. Code snippet of its use :
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c">#W is weight matrix obtained during this iteration</span>
  <span class="c">#dW - gradient of the weight matrix</span>
  <span class="c">#loss - loss seens during this iteration</span>
  <span class="c"># reg - regularisation strength</span>
  <span class="n">loss</span> <span class="o">+=</span> <span class="mf">0.5</span><span class="o">*</span><span class="n">reg</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W</span><span class="o">*</span><span class="n">W</span><span class="p">)</span>
  <span class="n">dW</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reg</span><span class="o">*</span><span class="n">W</span><span class="p">)</span>
  <span class="c">#=&gt; please note regularisation here is W*W and not reg. reg is the hyper parameter</span>
</code></pre></div>    </div>
  </li>
  <li>Number of Iterations - this is also hyper parameter as it depends on how long do you want to run your model.</li>
  <li>Many more based on the model you are using for eg. KNN has K as hyper parameter, etc.</li>
</ol>

<h3 id="cross-validation">Cross Validation</h3>
<p>This technique is used to remove overfitting in the learning algorithm. The training dataset is randomly partitioned in k-folds. One fold is chosen at random and other data is use for training. The algorithm is made to learn on the randomly chosen dataset and is validated on validation data set. This is performed k times.</p>

<p>There are two ways to go about now.</p>
<ol>
  <li>You can either select average of all the output of the k trials</li>
  <li>You can penalise those models which tries to overfit (better result on training dataset and bad result on validation dataset)</li>
</ol>

<h4 id="common-commands-in-jekyll-for-mac">Common commands in Jekyll for Mac:</h4>
<ol>
  <li>bundle exec jekyll serve</li>
  <li>open $(bundle show minima) -  if your theme is minima -  check config.yml file for it.</li>
</ol>

<!-- You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

To add new posts, simply add a file in the `_posts` directory that follows the convention `YYYY-MM-DD-name-of-post.ext` and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:


<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">print_hi</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
  <span class="k">print</span> <span class="p">(</span><span class="s">"Hi"</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>

<span class="n">print_hi</span><span class="p">(</span><span class="s">'Tom'</span><span class="p">)</span>
<span class="c">#=&gt; prints 'Hi, Tom' to STDOUT.</span></code></pre></figure>


Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ -->

  </div>

  
    


  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">My Experiments with Machine Learning</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              My Experiments with Machine Learning
            
            </li>
            
            <li><a href="mailto:chawla9@purdue.edu">chawla9@purdue.edu</a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/anieshchawla"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">anieshchawla</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/aniesh_chawla"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">aniesh_chawla</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>I started ML this year because of my interest and I wanted to share my experience with people who have been trying to work in this field. This is also a repository for me to revise concepts which I might forget.</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
