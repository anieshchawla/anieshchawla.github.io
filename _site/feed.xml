<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2017-11-17T23:34:33-05:00</updated><id>http://localhost:4000/</id><title type="html">My Experiments with Machine Learning</title><subtitle>I started ML this year because of my interest and I wanted to share my experience with people who have been trying to work in this field. This is also a repository for me to revise concepts which I might forget.</subtitle><entry><title type="html">Decision Trees</title><link href="http://localhost:4000/jekyll/update/2017/11/14/decision-trees.html" rel="alternate" type="text/html" title="Decision Trees" /><published>2017-11-14T00:15:12-05:00</published><updated>2017-11-14T00:15:12-05:00</updated><id>http://localhost:4000/jekyll/update/2017/11/14/decision-trees</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/11/14/decision-trees.html">&lt;p&gt;One of the most basic algorithm which is easier to understand yet very powerful is Decision Trees. There are two most popular algorithm associated with training the Decision Trees are :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;C4.5, ID3&lt;/li&gt;
  &lt;li&gt;CART&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;c45&quot;&gt;C4.5&lt;/h3&gt;
&lt;p&gt;This was developed by Quinlan and is extension of its earlier algorithm ID3. Consider Samples of data s1,s2, ….., sn. Each sample is k-length vector - which means that each sample is represted by k -columns or k variables.&lt;/p&gt;

&lt;p&gt;At each node C4.5 chooses the best attribute on which it can be split. The splitting criterion is information gain, which means that whichever attribute gives the largest information gain after splitting, that would be chosen. First, we should what is gain.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Gain(S,A) = H(S) - \sum_{a\subset A} \frac{|a|}{|S|} * H(a)&lt;/script&gt;

&lt;p&gt;where H is entropy, S is sample and A is the attribute chosen.
Entropy is given by :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(P) = \sum_i-p_i*log_{2}(p_i)&lt;/script&gt;

&lt;p&gt;Let’s take an example to make it more clear. The table below shows, age, income and whether that person buys a certain thing or not.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{|c|c|c|}
\hline
Age &amp; Income &amp; Buys \\ \hline
&lt;30&amp; high &amp; yes \\ \hline
&gt;30&amp; medium &amp; yes \\ \hline
&gt;30&amp; low &amp; no \\ \hline
&gt;30&amp; high &amp; no \\ \hline
&lt;30&amp; high &amp; yes \\ \hline
&lt;30&amp; medium &amp; yes \\ \hline
&gt;30&amp; high &amp; no \\ \hline
&lt;30&amp; low &amp; yes \\ \hline
&lt;30&amp; low &amp; yes \\ \hline
 \hline
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;1st we find the information stored in this table:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 H[buys]&amp; = -(probability of yes)* log_2(probability of yes) + -(probability of no)* log_2(probability of no) \\
&amp; = -6/9 * log_2(6/9) - 3/9*log_2(3/9) \\
&amp; = 0.91829
  \end {align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Lets consider now that we split based on income. So we have three categories - low(3), medium(2) and high(4).&lt;/p&gt;

&lt;p&gt;So Entropy associated with this attribute is as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;For high (2 yes and 2 no) : -2/4 log 2/4 -2/4 log 2/4 = 1&lt;/li&gt;
  &lt;li&gt;For medium (two yes) : -2/2 * log 2/2 = 0&lt;/li&gt;
  &lt;li&gt;For low (2 yes and 1 no) : -1/3 log (1/3) - 2/3 log(2/3) = 0.9182&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore, we can now find the gain as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
Gain &amp; = 0.91829 - (4/9 * (1) + 2/9 * 0 + 3/9 * 0.9182)\\
  &amp; = 0.167
  \end {align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similarly, the information gain for splitting on age attribute is = 0.5577. As we can see that the information gain is more in case of age, thus it will first choose to split via age attribute. Intuitively, this makes sense, if you see the table again, for age &amp;lt; 30 the person is buying the product.&lt;/p&gt;

&lt;h3 id=&quot;cart&quot;&gt;CART&lt;/h3&gt;

&lt;p&gt;This is similar to C4.5 but uses Gini index instead of entropy for gain. Gini index is defined as :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Gini(X) = 1 - \sum_x p(x)^2&lt;/script&gt;

&lt;p&gt;And there Gain is defined in this case as :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Gain(S,A) = Gini(S) - \sum_{a\subset A} \frac{|a|}{|S|} * Gini(a)&lt;/script&gt;

&lt;h3 id=&quot;when-should-we-stop-growing-the-tree&quot;&gt;When Should we stop growing the Tree?&lt;/h3&gt;
&lt;p&gt;We should be vary of the depth of the decision tree that we are making. More depth generally means that we are overfitting the data. There are two basic ways to avoid overfitting in Decision Trees:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pre - Pruning - as the name suggests, we start making decision trees only after making some initial decisions. It can have two major criteria :
    &lt;ul&gt;
      &lt;li&gt;Depth -  which means that we define the maximum depth till which we will allow tree to grow&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;Pre defined Conditions - which means, we will make certain conditions and can make our decision trees grow to certain depth till that condition is reached on the leaf node - Following are couple of examples of such pruning conditions:
        &lt;ul&gt;
          &lt;li&gt;if all attribute value on the leaf node are same&lt;/li&gt;
          &lt;li&gt;number of attributes in the leaf node are x (where x &amp;lt; total attributes provided initially)&lt;/li&gt;
          &lt;li&gt;attributes reaches certain threshold values&lt;/li&gt;
          &lt;li&gt;Chi-square feature is not statistically significant&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Post - Pruning - This is pruning mechanism in a bottoms-up approach.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;C4.5 and CART both uses different pruning mechanism to remove overfitting. C4.5 uses “Reduce Error Pruning” whereas CART uses “Cross validation for selecting Gini Threshold”&lt;/p&gt;

&lt;h3 id=&quot;reduce-error-pruning&quot;&gt;Reduce Error Pruning&lt;/h3&gt;
&lt;p&gt;In this mechanism, We start pruning in an bottom’s up approach in the following fashion:
Let T be subtree whose root node is v. We define gain from pruning as :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Gain(pruning) = \#misclassification_T - misclassification_v&lt;/script&gt;

&lt;p&gt;If this gain is +ve, it means that there were fewer misclassification when we started with and have more misclassification in the subtree T. Thus we will start removing those nodes which have Gain +ve and thereby keeping only -ve pruning gain nodes.&lt;/p&gt;

&lt;h3 id=&quot;cross-validation-for-selecting-gini-threshold&quot;&gt;Cross validation for selecting Gini Threshold&lt;/h3&gt;

&lt;p&gt;For cross validation technique you can look at &lt;a href=&quot;http://localhost:4000//jekyll/update/2017/11/11/important-terms.html&quot;&gt;my previous post&lt;/a&gt;. This is pre-pruning mechanism with condition testing. In this method, for each validation test set, define a Gini threshold &lt;script type=&quot;math/tex&quot;&gt;G_t&lt;/script&gt; for the decision tree. This means that whenever the leaf node reaches Gini index at &lt;script type=&quot;math/tex&quot;&gt;G_t&lt;/script&gt;, you should stop. The best &lt;script type=&quot;math/tex&quot;&gt;G_t&lt;/script&gt; is chosen from these validation sets and is finally used during testing.&lt;/p&gt;</content><author><name></name></author><summary type="html">One of the most basic algorithm which is easier to understand yet very powerful is Decision Trees. There are two most popular algorithm associated with training the Decision Trees are : C4.5, ID3 CART</summary></entry><entry><title type="html">Starting Terms ML</title><link href="http://localhost:4000/jekyll/update/2017/11/11/important-terms.html" rel="alternate" type="text/html" title="Starting Terms ML" /><published>2017-11-11T00:08:12-05:00</published><updated>2017-11-11T00:08:12-05:00</updated><id>http://localhost:4000/jekyll/update/2017/11/11/important-terms</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/11/11/important-terms.html">&lt;p&gt;We will start with some very common terms that have find it use in ML&lt;/p&gt;

&lt;h3 id=&quot;loss-functions-&quot;&gt;Loss functions :&lt;/h3&gt;
&lt;p&gt;This is finally what you would expect to be low. There are many loss functions that are used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L1 Loss - This is basically absolute value of difference of input and output (I have never used) :&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Loss = \sum_{i=0}^n |y_i - f(x_i)|&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;L2 Loss - This is square of difference of input and output (used extensively) :&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Loss = \sum_{i=0}^n (y_i - f(x_i))^2&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Softmax loss - This is loss that comes from softmax classifiers (will discuss it in next post)
  basic equation for this is given by following equation. If &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is the expected output for the given input.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Loss = -\log(\frac{\exp(y_i)}{\sum_{k=0}^n \exp(y_k)})&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;  &lt;span class=&quot;c&quot;&gt;# Code For Softmax Loss function calculation - used very frequently in Deep Neural Networks&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Training Inputs:&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# W: A numpy array of shape (D, C) containing weights.&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# X: A numpy array of shape (N, D) containing a minibatch of data.&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# y: A numpy array of shape (N,) containing training labels; y[i] = c means&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# that X[i] has label c, where 0 &amp;lt;= c &amp;lt; C.&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# doing the following to maintain numerical stability&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;expscore&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expscore&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;#=&amp;gt; Please note this is not the final loss output - have to take mean and take into account regularisation&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;#=&amp;gt; Gives non-vectorized loss for softmax classifier&lt;/span&gt;
   &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Since we are using Numpy, we should do this in much smarter way i.e. vectorized solution&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;      &lt;span class=&quot;c&quot;&gt;#Inputs are same as above&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# following is done to maintain the numerical stability by subtracting the max values&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;expval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;#scores[np.arange(len(scores)),y] - this is basically taking the values only belonging to classifier&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;#=&amp;gt; Gives vectorized loss for softmax&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;#=&amp;gt; Please note this is not the final loss output - have to take into account regularisation&lt;/span&gt;
      &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;hyper-parameters&quot;&gt;Hyper parameters:&lt;/h3&gt;

&lt;p&gt;The parameters that are not learned by during the training process but are required to make your model
learn and converge. Some of the common hyper parameters are :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Learning rate - This tell how fast do you want your model to train. A high value can explode your data
very quickly and a low value will lead to very slow convergence. To select the right value, a number of
iterations are required. A good value to start with is 1e-3. A snippet of how to use learning rate:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;c&quot;&gt;#self.W is weight matrix&lt;/span&gt;
 &lt;span class=&quot;c&quot;&gt;#grad is gradient for iteration - will talk more about it on later posts&lt;/span&gt;
 &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Regularisation Strength - Regularisation is responsible to keep your model simple and not to become complex. Since we are dealing with non-linear functions, we need to keep the keep the model in check to not to become super
complex. Generally we use L2 regularization
&lt;script type=&quot;math/tex&quot;&gt;\sum_i\sum_k W_iW_k&lt;/script&gt;. A good value to start with for regularisation strength is 1e-5. Code snippet of its use :
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;c&quot;&gt;#W is weight matrix obtained during this iteration&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#dW - gradient of the weight matrix&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#loss - loss seens during this iteration&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# reg - regularisation strength&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#=&amp;gt; please note regularisation here is W*W and not reg. reg is the hyper parameter&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Number of Iterations - this is also hyper parameter as it depends on how long do you want to run your model.&lt;/li&gt;
  &lt;li&gt;Many more based on the model you are using for eg. KNN has K as hyper parameter, etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;cross-validation&quot;&gt;Cross Validation&lt;/h3&gt;
&lt;p&gt;This technique is used to remove overfitting in the learning algorithm. The training dataset is randomly partitioned in k-folds. One fold is chosen at random and other data is use for training. The algorithm is made to learn on the randomly chosen dataset and is validated on validation data set. This is performed k times.&lt;/p&gt;

&lt;p&gt;There are two ways to go about now.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;You can either select average of all the output of the k trials&lt;/li&gt;
  &lt;li&gt;You can penalise those models which tries to overfit (better result on training dataset and bad result on validation dataset)&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;common-commands-in-jekyll-for-mac&quot;&gt;Common commands in Jekyll for Mac:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;bundle exec jekyll serve&lt;/li&gt;
  &lt;li&gt;open $(bundle show minima) -  if your theme is minima -  check config.yml file for it.&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

To add new posts, simply add a file in the `_posts` directory that follows the convention `YYYY-MM-DD-name-of-post.ext` and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:


&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hi&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;


Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ --&gt;</content><author><name></name></author><summary type="html">We will start with some very common terms that have find it use in ML</summary></entry></feed>