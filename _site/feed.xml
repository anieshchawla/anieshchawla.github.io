<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-03-29T01:35:58-04:00</updated><id>http://localhost:4000/</id><title type="html">My Experiments with Machine Learning</title><subtitle>I started ML this year because of my interest and I wanted to share my experience with people who have been trying to work in this field. This is also a repository for me to revise concepts which I might forget.</subtitle><entry><title type="html">Random Forest</title><link href="http://localhost:4000/jekyll/update/2018/01/13/Random-Forest.html" rel="alternate" type="text/html" title="Random Forest" /><published>2018-01-13T00:15:12-05:00</published><updated>2018-01-13T00:15:12-05:00</updated><id>http://localhost:4000/jekyll/update/2018/01/13/Random-Forest</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/01/13/Random-Forest.html">&lt;p&gt;This algorithm belong the category of &lt;strong&gt;supervised learning&lt;/strong&gt; algorithms. This in a very crude way be considered as ensemble classifier for the Decision Trees. Just like Decision Trees, this algorithm can be used both for classification as well as regression. Please read about &lt;a href=&quot;http://localhost:4000//jekyll/update/2017/11/14/decision-trees.html&quot;&gt;Decision Trees&lt;/a&gt; before we get on to Random Forest.&lt;/p&gt;

&lt;h3 id=&quot;random-forest&quot;&gt;Random Forest&lt;/h3&gt;
&lt;p&gt;As the name suggests, this is a forest i.e. group of many decision trees. The &lt;code class=&quot;highlighter-rouge&quot;&gt;Random&lt;/code&gt; in  Random Forest is the random selection of parameters at each node, we will see in a while what does it mean. Random Forest is a very powerful algorithm. When I was working in American Express in fraud analytics team, we shifted our focus from building traditional Linear Regression model to building models using this algorithm. We also used this  for fraud classification and building rules.&lt;/p&gt;

&lt;h3 id=&quot;algorithm&quot;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;The algorithm is very simple. But first we will assume the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The depth of decision tree used in Random Forest is a hyper parameter.&lt;/li&gt;
  &lt;li&gt;Each data point has k parameters in total&lt;/li&gt;
  &lt;li&gt;There are N Decision Trees that we expect in this algorithm. This is also a hyper parameter.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The algorithm is as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Select m parameters out of k parameters, where m « k.&lt;/li&gt;
  &lt;li&gt;Select the best parameter out of these &lt;code class=&quot;highlighter-rouge&quot;&gt;m&lt;/code&gt; to get the best split at each node. The best split selection is in the same way as in &lt;a href=&quot;http://localhost:4000//jekyll/update/2017/11/14/decision-trees.html&quot;&gt;Decision Trees&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Continue the process from 1-2 till you reach the depth of decision tree.&lt;/li&gt;
  &lt;li&gt;Do 1-3 for each Decision Tree in Random Forest.&lt;/li&gt;
  &lt;li&gt;Select the most frequent outcome of all the Trees.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;other-benefits&quot;&gt;Other Benefits&lt;/h2&gt;
&lt;p&gt;After the algorithm is complete. You can also rank order the number of parameter which appeared during the split based on the times they are used for finding the best split. When I was working in fraud analytics team in American Express, we often used the parameters that came in rank order to create rules for finding frauds. We directly couldn’t use the results from Random Forest because we had to justify the parameters used for creating rule not only to the senior leaders but also to the auditing firms.&lt;/p&gt;

&lt;h2 id=&quot;implementation-in-python&quot;&gt;Implementation in python&lt;/h2&gt;
&lt;p&gt;Implementation of this algorithm is very simple in python. We will not be reinventing the wheel again and will use the algorithms already present in various open source libraries. In this case we will be using pandas and scikit learn packages of python. Please see the implementation below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.ensemble&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#Lets suppose dataset is complete without any missing data. In my dataset that I have removed the rows which had missing entries.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CSV_FILE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#features: These are the columns which have the variables for the dataset&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#target: This column is what we want as our output&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#percentageSplit: this is the percentage we want our dataset to be split into train and test - I chose 67%&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;percentageSplit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#initialize a classifier&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rf_classfier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#train the classifier with our data set&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rf_classfier&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#predict the output with the trained model&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_classf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#now we will see the result using accuracy_score&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Train Accuracy: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rf_classf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Test Accuracy: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;#confusion matrix can also be found using the following command&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Confusion Matrix: &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;confusion_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">This algorithm belong the category of supervised learning algorithms. This in a very crude way be considered as ensemble classifier for the Decision Trees. Just like Decision Trees, this algorithm can be used both for classification as well as regression. Please read about Decision Trees before we get on to Random Forest.</summary></entry><entry><title type="html">Decision Trees</title><link href="http://localhost:4000/jekyll/update/2017/11/14/decision-trees.html" rel="alternate" type="text/html" title="Decision Trees" /><published>2017-11-14T00:15:12-05:00</published><updated>2017-11-14T00:15:12-05:00</updated><id>http://localhost:4000/jekyll/update/2017/11/14/decision-trees</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/11/14/decision-trees.html">&lt;p&gt;One of the most basic algorithm which is easier to understand yet very powerful is Decision Trees. There are two most popular algorithm associated with training the Decision Trees are :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;C4.5, ID3&lt;/li&gt;
  &lt;li&gt;CART&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;c45&quot;&gt;C4.5&lt;/h3&gt;
&lt;p&gt;This was developed by Quinlan and is extension of its earlier algorithm ID3. Consider Samples of data s1,s2, ….., sn. Each sample is k-length vector - which means that each sample is represented by k -columns or k variables.&lt;/p&gt;

&lt;p&gt;At each node C4.5 chooses the best attribute on which it can be split. The splitting criterion is information gain, which means that whichever attribute gives the largest information gain after splitting, that would be chosen. First, we should understand what is gain.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Gain(S,A) = H(S) - \sum_{a\subset A} \frac{|a|}{|S|} * H(a)&lt;/script&gt;

&lt;p&gt;where H is entropy, S is sample and A is the attribute chosen.
Entropy is given by :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(P) = \sum_i-p_i*log_{2}(p_i)&lt;/script&gt;

&lt;p&gt;Let’s take an example to make it more clear. The table below shows, age, income and whether that person buys a certain thing or not.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{|c|c|c|}
\hline
Age &amp; Income &amp; Buys \\ \hline
&lt;30&amp; high &amp; yes \\ \hline
&gt;30&amp; medium &amp; yes \\ \hline
&gt;30&amp; low &amp; no \\ \hline
&gt;30&amp; high &amp; no \\ \hline
&lt;30&amp; high &amp; yes \\ \hline
&lt;30&amp; medium &amp; yes \\ \hline
&gt;30&amp; high &amp; no \\ \hline
&lt;30&amp; low &amp; yes \\ \hline
&lt;30&amp; low &amp; yes \\ \hline
 \hline
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;1st we find the information stored in this table:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 H[buys]&amp; = -(probability of yes)* log_2(probability of yes) + -(probability of no)* log_2(probability of no) \\
&amp; = -6/9 * log_2(6/9) - 3/9*log_2(3/9) \\
&amp; = 0.91829
  \end {align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Lets consider now that we split based on income. So we have three categories - low(3), medium(2) and high(4).&lt;/p&gt;

&lt;p&gt;So Entropy associated with income attribute is as follows:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;For high (2 yes and 2 no) : -2/4 log 2/4 -2/4 log 2/4 = 1&lt;/li&gt;
  &lt;li&gt;For medium (two yes) : -2/2 * log 2/2 = 0&lt;/li&gt;
  &lt;li&gt;For low (2 yes and 1 no) : -1/3 log (1/3) - 2/3 log(2/3) = 0.9182&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Therefore, we can now find the gain as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
Gain &amp; = 0.91829 - (4/9 * (1) + 2/9 * 0 + 3/9 * 0.9182)\\
  &amp; = 0.167
  \end {align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Similarly, the information gain for splitting on age attribute is = 0.8091. As we can see that the information gain is more in case of age, thus it will first choose to split via age attribute. Intuitively, this makes sense, if you see the table again, for age &amp;lt; 30 the person is buying the product.&lt;/p&gt;

&lt;h3 id=&quot;cart&quot;&gt;CART&lt;/h3&gt;

&lt;p&gt;This is similar to C4.5 but uses Gini index instead of entropy for gain. Gini index is defined as :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Gini(X) = 1 - \sum_x p(x)^2&lt;/script&gt;

&lt;p&gt;And Gain for using a particular attribute &lt;code class=&quot;highlighter-rouge&quot;&gt;A&lt;/code&gt; is defined as :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Gain(S,A) = Gini(S) - \sum_{a\subset A} \frac{|a|}{|S|} * Gini(a)&lt;/script&gt;

&lt;h3 id=&quot;when-should-we-stop-growing-the-tree&quot;&gt;When Should we stop growing the Tree?&lt;/h3&gt;
&lt;p&gt;We should be vary of the depth of the decision tree that we are making. More depth generally means that we are overfitting the data. There are two basic ways to avoid overfitting in Decision Trees:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Pre - Pruning - as the name suggests, we start making decision trees only after making some initial decisions. It can have two major criteria :
    &lt;ul&gt;
      &lt;li&gt;Depth -  which means that we define the maximum depth till which we will allow tree to grow&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;Pre defined Conditions - which means, we will make certain conditions and can make our decision trees grow to certain depth till that condition is reached on the leaf node - Following are couple of examples of such pruning conditions:
        &lt;ul&gt;
          &lt;li&gt;if all attribute value on the leaf node are same&lt;/li&gt;
          &lt;li&gt;number of attributes in the leaf node are x (where x &amp;lt; total attributes provided initially)&lt;/li&gt;
          &lt;li&gt;attributes reaches certain threshold values&lt;/li&gt;
          &lt;li&gt;Chi-square feature is not statistically significant&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Post - Pruning - This is pruning mechanism in a bottoms-up approach.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;C4.5 and CART both uses different pruning mechanism to remove overfitting. C4.5 uses “Reduce Error Pruning” whereas CART uses “Cross validation for selecting Gini Threshold”&lt;/p&gt;

&lt;h3 id=&quot;reduce-error-pruning&quot;&gt;Reduce Error Pruning&lt;/h3&gt;
&lt;p&gt;In this mechanism, We start pruning in an bottom’s up approach in the following fashion:
Let T be subtree whose root node is v. We define gain from pruning as :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Gain(pruning) = \#misclassification_T - misclassification_v&lt;/script&gt;

&lt;p&gt;If this gain is +ve, it means that there were fewer misclassification when we started with and have more misclassification in the subtree T. Thus we will start removing those nodes which have Gain +ve and thereby keeping only -ve pruning gain nodes.&lt;/p&gt;

&lt;h3 id=&quot;cross-validation-for-selecting-gini-threshold&quot;&gt;Cross validation for selecting Gini Threshold&lt;/h3&gt;

&lt;p&gt;For cross validation technique you can look at &lt;a href=&quot;http://localhost:4000//jekyll/update/2017/11/11/important-terms.html&quot;&gt;my previous post&lt;/a&gt;. This is pre-pruning mechanism with condition testing. In this method, for each validation test set, define a Gini threshold &lt;script type=&quot;math/tex&quot;&gt;G_t&lt;/script&gt; for the decision tree. This means that whenever the leaf node reaches Gini index at &lt;script type=&quot;math/tex&quot;&gt;G_t&lt;/script&gt;, you should stop. The best &lt;script type=&quot;math/tex&quot;&gt;G_t&lt;/script&gt; is chosen from these validation sets and is finally used during testing.&lt;/p&gt;</content><author><name></name></author><summary type="html">One of the most basic algorithm which is easier to understand yet very powerful is Decision Trees. There are two most popular algorithm associated with training the Decision Trees are : C4.5, ID3 CART</summary></entry><entry><title type="html">Starting Terms ML</title><link href="http://localhost:4000/jekyll/update/2017/11/11/important-terms.html" rel="alternate" type="text/html" title="Starting Terms ML" /><published>2017-11-11T00:08:12-05:00</published><updated>2017-11-11T00:08:12-05:00</updated><id>http://localhost:4000/jekyll/update/2017/11/11/important-terms</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2017/11/11/important-terms.html">&lt;p&gt;We will start with some very common terms that have find it use in ML&lt;/p&gt;

&lt;h3 id=&quot;loss-functions-&quot;&gt;Loss functions :&lt;/h3&gt;
&lt;p&gt;This is finally what you would expect to be low. There are many loss functions that are used:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;L1 Loss - This is basically absolute value of difference of input and output (I have never used) :&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Loss = \sum_{i=0}^n |y_i - f(x_i)|&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;L2 Loss - This is square of difference of input and output (used extensively) :&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Loss = \sum_{i=0}^n (y_i - f(x_i))^2&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Softmax loss - This is loss that comes from softmax classifiers (will discuss it in next post)
  basic equation for this is given by following equation. If &lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt; is the expected output for the given input.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Loss = -\log(\frac{\exp(y_i)}{\sum_{k=0}^n \exp(y_k)})&lt;/script&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;  &lt;span class=&quot;c&quot;&gt;# Code For Softmax Loss function calculation - used very frequently in Deep Neural Networks&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# Training Inputs:&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# W: A numpy array of shape (D, C) containing weights.&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# X: A numpy array of shape (N, D) containing a minibatch of data.&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# y: A numpy array of shape (N,) containing training labels; y[i] = c means&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;# that X[i] has label c, where 0 &amp;lt;= c &amp;lt; C.&lt;/span&gt;
   &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# doing the following to maintain numerical stability&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;expscore&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expscore&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;#=&amp;gt; Please note this is not the final loss output - have to take mean and take into account regularisation&lt;/span&gt;
   &lt;span class=&quot;c&quot;&gt;#=&amp;gt; Gives non-vectorized loss for softmax classifier&lt;/span&gt;
   &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Since we are using Numpy, we should do this in much smarter way i.e. vectorized solution&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;      &lt;span class=&quot;c&quot;&gt;#Inputs are same as above&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;# following is done to maintain the numerical stability by subtracting the max values&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;expval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transpose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;#scores[np.arange(len(scores)),y] - this is basically taking the values only belonging to classifier&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;#=&amp;gt; Gives vectorized loss for softmax&lt;/span&gt;
      &lt;span class=&quot;c&quot;&gt;#=&amp;gt; Please note this is not the final loss output - have to take into account regularisation&lt;/span&gt;
      &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;hyper-parameters&quot;&gt;Hyper parameters:&lt;/h3&gt;

&lt;p&gt;The parameters that are not learned by during the training process but are required to make your model
learn and converge. Some of the common hyper parameters are :&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Learning rate - This tell how fast do you want your model to train. A high value can explode your data
very quickly and a low value will lead to very slow convergence. To select the right value, a number of
iterations are required. A good value to start with is 1e-3. A snippet of how to use learning rate:
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; &lt;span class=&quot;c&quot;&gt;#self.W is weight matrix&lt;/span&gt;
 &lt;span class=&quot;c&quot;&gt;#grad is gradient for iteration - will talk more about it on later posts&lt;/span&gt;
 &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt;  &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Regularisation Strength - Regularisation is responsible to keep your model simple and not to become complex. Since we are dealing with non-linear functions, we need to keep the keep the model in check to not to become super
complex. Generally we use L2 regularization
&lt;script type=&quot;math/tex&quot;&gt;\sum_i\sum_k W_iW_k&lt;/script&gt;. A good value to start with for regularisation strength is 1e-5. Code snippet of its use :
    &lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  &lt;span class=&quot;c&quot;&gt;#W is weight matrix obtained during this iteration&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#dW - gradient of the weight matrix&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#loss - loss seens during this iteration&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;# reg - regularisation strength&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c&quot;&gt;#=&amp;gt; please note regularisation here is W*W and not reg. reg is the hyper parameter&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Number of Iterations - this is also hyper parameter as it depends on how long do you want to run your model.&lt;/li&gt;
  &lt;li&gt;Many more based on the model you are using for eg. KNN has K as hyper parameter, etc.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;cross-validation&quot;&gt;Cross Validation&lt;/h3&gt;
&lt;p&gt;This technique is used to remove overfitting in the learning algorithm. The training dataset is randomly partitioned in k-folds. One fold is chosen at random and other data is use for training. The algorithm is made to learn on the randomly chosen dataset and is validated on validation data set. This is performed k times.&lt;/p&gt;

&lt;p&gt;There are two ways to go after cross validation:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;You can either select average of all the output of the &lt;code class=&quot;highlighter-rouge&quot;&gt;k&lt;/code&gt; trials&lt;/li&gt;
  &lt;li&gt;You can penalise those models which tries to overfit (better result on training dataset and bad result on validation dataset)&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;performance&quot;&gt;Performance&lt;/h3&gt;
&lt;p&gt;Performance of any machine learning algorithm generally uses terms like &lt;code class=&quot;highlighter-rouge&quot;&gt;confusion matrix&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;precision&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;recall&lt;/code&gt;, etc. We will understand their meaning by taking a simple example. Lets suppose our algorithm predicts whether a particular credit card transaction is fraud or not. We ran our algorithm on 20 transactions, out of which it predicted 4 being as fraud and 16 as not fraud. Out of the 4 transactions predicted, only 3 were actually fraud. We know what there were total of 6 fraud transactions and 14 regular transactions.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{|c|c|c|}
\hline
 n=&amp; Predicted &amp; Predicted \\
 20&amp; Fraud(4) &amp; \text{Not Fraud(16)} \\ \hline
\text{Actual Fraud(6)}&amp; 3 &amp; 3 \\ \hline
\text{Actual Not Fraud(14)} &amp; 1 &amp; 13 \\ \hline
 \hline
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Now we will define our those terms:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Confusion Matrix&lt;/strong&gt; : The matrix shown above is the confusion matrix. This is the main performance matrix for classification algorithms.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Precision&lt;/strong&gt;: As the name suggests, it tells how precise is the algorithm. In our case it is ratio of (actual fraud in predicted)/ (total fraud predicted) i.e 3/4. In technical terms it is ratio of true positives to total predicted. That is why Precision is also called positive predictive value.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Recall&lt;/strong&gt;: This term tell us about the sensitivity of prediction. It tell us about how much was the algorithm actually able to predict properly. In our case it is ratio of (actual fraud in predicted)/(total fraud in transactions) i.e. 3/6 = 1/2. Technically, it is ratio of true positives to all positive&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;True Positive Rate&lt;/strong&gt;: When it is fraud, how often does the algorithm predict a transaction to be fraud. In our example, it is 3/6.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;False Negative Rate&lt;/strong&gt;: When it is not fraud, how often does the algorithm predict a transaction to be fraud. In our example, it is 1/13.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Accuracy&lt;/strong&gt;: This tells us, how accurate our algorithm is. In our case it is (3+13)/20 = 80%.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Misclassification Rate&lt;/strong&gt;: This is opposite of Accuracy i.e. missclass_rate = 1 - accuracy = 20%.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prevalance&lt;/strong&gt;: It tells us the whether our data is sparse or not for classification. In our example it is percentage of transaction that are fraud in dataset i.e. 6/20 = 30%.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;common-commands-in-jekyll-for-mac&quot;&gt;Common commands in Jekyll for Mac:&lt;/h4&gt;
&lt;ol&gt;
  &lt;li&gt;bundle exec jekyll serve&lt;/li&gt;
  &lt;li&gt;open $(bundle show minima) -  if your theme is minima -  check config.yml file for it.&lt;/li&gt;
&lt;/ol&gt;

&lt;!-- You’ll find this post in your `_posts` directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run `jekyll serve`, which launches a web server and auto-regenerates your site when a file is updated.

To add new posts, simply add a file in the `_posts` directory that follows the convention `YYYY-MM-DD-name-of-post.ext` and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.

Jekyll also offers powerful support for code snippets:


&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Hi&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Tom'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#=&amp;gt; prints 'Hi, Tom' to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;


Check out the [Jekyll docs][jekyll-docs] for more info on how to get the most out of Jekyll. File all bugs/feature requests at [Jekyll’s GitHub repo][jekyll-gh]. If you have questions, you can ask them on [Jekyll Talk][jekyll-talk].

[jekyll-docs]: https://jekyllrb.com/docs/home
[jekyll-gh]:   https://github.com/jekyll/jekyll
[jekyll-talk]: https://talk.jekyllrb.com/ --&gt;</content><author><name></name></author><summary type="html">We will start with some very common terms that have find it use in ML</summary></entry></feed>